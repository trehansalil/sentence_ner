{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced NER Model Training\n",
    "\n",
    "This notebook implements and trains an advanced NER model using BiLSTM with attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "from advanced_model import AdvancedNERModel, create_advanced_model, train_advanced_model\n",
    "from data_preprocessing import NERDataProcessor\n",
    "from evaluation import NERModelEvaluator, evaluate_predictions\n",
    "from utils import load_results, save_results\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Configure TensorFlow\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "print(\"Loading preprocessed data...\")\n",
    "\n",
    "# Load the numpy arrays\n",
    "data = np.load('../results/processed_data.npz')\n",
    "X_train = data['X_train']\n",
    "X_val = data['X_val']\n",
    "X_test = data['X_test']\n",
    "y_train = data['y_train']\n",
    "y_val = data['y_val']\n",
    "y_test = data['y_test']\n",
    "\n",
    "# Load metadata\n",
    "metadata = load_results('../results/preprocessing_metadata.json')\n",
    "\n",
    "print(f\"Data loaded successfully!\")\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Vocabulary size: {metadata['vocab_size']:,}\")\n",
    "print(f\"Number of tags: {metadata['num_tags']}\")\n",
    "print(f\"Max sequence length: {metadata['max_sequence_length']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Advanced Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "model_params = {\n",
    "    'embedding_dim': 200,\n",
    "    'lstm_units': 128,\n",
    "    'num_attention_heads': 8,\n",
    "    'dropout_rate': 0.3,\n",
    "    'use_crf': False  # Set to True if tensorflow-addons is available\n",
    "}\n",
    "\n",
    "print(f\"Creating advanced model with parameters:\")\n",
    "for param, value in model_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# Create the advanced model\n",
    "advanced_model = create_advanced_model(\n",
    "    vocab_size=metadata['vocab_size'],\n",
    "    num_tags=metadata['num_tags'],\n",
    "    max_sequence_length=metadata['max_sequence_length'],\n",
    "    **model_params\n",
    ")\n",
    "\n",
    "print(\"\\nAdvanced model created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and display model architecture\n",
    "advanced_model.build_simple_bilstm_model()  # Use simpler BiLSTM for stability\n",
    "print(\"Model Architecture:\")\n",
    "print(advanced_model.get_model_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the Advanced Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "training_params = {\n",
    "    'epochs': 50,\n",
    "    'batch_size': 32,\n",
    "    'patience': 15\n",
    "}\n",
    "\n",
    "print(f\"Training parameters:\")\n",
    "for param, value in training_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Train the model\n",
    "training_results = advanced_model.train(\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    model_save_path='../models/advanced_model.h5',\n",
    "    **training_params\n",
    ")\n",
    "\n",
    "end_time = datetime.now()\n",
    "training_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "print(f\"\\nTraining completed in {training_time:.1f} seconds ({training_time/60:.1f} minutes)\")\n",
    "print(f\"Epochs trained: {training_results['epochs_trained']}\")\n",
    "print(f\"Best validation accuracy: {training_results['best_val_accuracy']:.4f}\")\n",
    "print(f\"Best validation loss: {training_results['best_val_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "advanced_model.plot_training_history(save_path='../results/visualizations/advanced_training_history.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate the Advanced Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating advanced model on test set...\")\n",
    "\n",
    "# Basic evaluation metrics\n",
    "test_metrics = advanced_model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_metrics['test_loss']:.4f}\")\n",
    "print(f\"Test Accuracy: {test_metrics['test_accuracy']:.4f}\")\n",
    "\n",
    "# Get predictions\n",
    "print(\"\\nGenerating predictions...\")\n",
    "y_pred = advanced_model.predict(X_test)\n",
    "print(f\"Predictions shape: {y_pred.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive evaluation using our custom evaluator\n",
    "print(\"Performing comprehensive evaluation...\")\n",
    "\n",
    "# Create evaluator\n",
    "id_to_tag = {int(k): v for k, v in metadata['id_to_tag'].items()}\n",
    "evaluator = NERModelEvaluator(id_to_tag)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation_results = evaluator.evaluate_model(y_test, y_pred, X_test)\n",
    "\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Token-level metrics\n",
    "token_metrics = evaluation_results['token_level']\n",
    "print(f\"Token-level Accuracy: {token_metrics['accuracy']:.4f}\")\n",
    "print(f\"Token-level Precision: {token_metrics['precision']:.4f}\")\n",
    "print(f\"Token-level Recall: {token_metrics['recall']:.4f}\")\n",
    "print(f\"Token-level F1-Score: {token_metrics['f1_score']:.4f}\")\n",
    "\n",
    "# Sequence-level metrics\n",
    "seq_metrics = evaluation_results['sequence_level']\n",
    "print(f\"\\nSequence-level Accuracy: {seq_metrics['sequence_accuracy']:.4f}\")\n",
    "print(f\"Exact Matches: {seq_metrics['exact_matches']}/{seq_metrics['total_sequences']}\")\n",
    "\n",
    "# Entity-level metrics\n",
    "entity_metrics = evaluation_results['entity_level']\n",
    "print(f\"\\nEntity-level Precision: {entity_metrics['precision']:.4f}\")\n",
    "print(f\"Entity-level Recall: {entity_metrics['recall']:.4f}\")\n",
    "print(f\"Entity-level F1-Score: {entity_metrics['f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-tag performance analysis\n",
    "per_tag_metrics = evaluation_results['per_tag']\n",
    "print(\"\\nPer-Tag Performance:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Convert to DataFrame for better display\n",
    "per_tag_df = pd.DataFrame(per_tag_metrics).T\n",
    "per_tag_df = per_tag_df.sort_values('f1_score', ascending=False)\n",
    "\n",
    "print(per_tag_df.round(4))\n",
    "\n",
    "# Plot per-tag F1 scores\n",
    "evaluator.plot_per_tag_metrics(per_tag_metrics, metric='f1_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "confusion_matrix = evaluation_results['confusion_matrix']\n",
    "evaluator.plot_confusion_matrix(confusion_matrix, title=\"Advanced Model - Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification report\n",
    "y_true_flat = evaluator._flatten_sequences(y_test)\n",
    "y_pred_flat = evaluator._flatten_sequences(y_pred)\n",
    "\n",
    "classification_report = evaluator.generate_classification_report(y_true_flat, y_pred_flat)\n",
    "print(\"Detailed Classification Report:\")\n",
    "print(\"=\" * 40)\n",
    "print(classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare with Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline results for comparison\n",
    "try:\n",
    "    baseline_results = load_results('../results/baseline_results.json')\n",
    "    \n",
    "    print(\"Model Comparison:\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Prepare comparison data\n",
    "    comparison_data = [\n",
    "        {\n",
    "            'Model': 'Baseline',\n",
    "            'Token Accuracy': baseline_results['evaluation_results']['token_level']['accuracy'],\n",
    "            'Token F1': baseline_results['evaluation_results']['token_level']['f1_score'],\n",
    "            'Sequence Accuracy': baseline_results['evaluation_results']['sequence_level']['sequence_accuracy'],\n",
    "            'Entity F1': baseline_results['evaluation_results']['entity_level']['f1_score'],\n",
    "            'Training Time (min)': baseline_results['model_info']['training_time_seconds'] / 60\n",
    "        },\n",
    "        {\n",
    "            'Model': 'Advanced',\n",
    "            'Token Accuracy': token_metrics['accuracy'],\n",
    "            'Token F1': token_metrics['f1_score'],\n",
    "            'Sequence Accuracy': seq_metrics['sequence_accuracy'],\n",
    "            'Entity F1': entity_metrics['f1_score'],\n",
    "            'Training Time (min)': training_time / 60\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    print(comparison_df.round(4))\n",
    "    \n",
    "    # Calculate improvements\n",
    "    improvements = {\n",
    "        'Token Accuracy': (token_metrics['accuracy'] - baseline_results['evaluation_results']['token_level']['accuracy']) * 100,\n",
    "        'Token F1': (token_metrics['f1_score'] - baseline_results['evaluation_results']['token_level']['f1_score']) * 100,\n",
    "        'Sequence Accuracy': (seq_metrics['sequence_accuracy'] - baseline_results['evaluation_results']['sequence_level']['sequence_accuracy']) * 100,\n",
    "        'Entity F1': (entity_metrics['f1_score'] - baseline_results['evaluation_results']['entity_level']['f1_score']) * 100\n",
    "    }\n",
    "    \n",
    "    print(\"\\nImprovements (percentage points):\")\n",
    "    for metric, improvement in improvements.items():\n",
    "        print(f\"  {metric}: {improvement:+.2f}%\")\n",
    "    \n",
    "    # Plot comparison\n",
    "    evaluator.plot_model_comparison(comparison_df)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Baseline results not found. Run the baseline model notebook first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prediction errors\n",
    "print(\"Error Analysis:\")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "# Find sequences with lowest prediction accuracy\n",
    "sequence_accuracies = []\n",
    "for i in range(len(y_test)):\n",
    "    true_seq = y_test[i]\n",
    "    pred_seq = y_pred[i]\n",
    "    \n",
    "    # Calculate accuracy for this sequence (ignoring padding)\n",
    "    non_pad_mask = true_seq != 0  # Assuming 0 is padding\n",
    "    if non_pad_mask.sum() > 0:\n",
    "        seq_acc = (true_seq[non_pad_mask] == pred_seq[non_pad_mask]).mean()\n",
    "        sequence_accuracies.append((i, seq_acc))\n",
    "\n",
    "# Sort by accuracy\n",
    "sequence_accuracies.sort(key=lambda x: x[1])\n",
    "\n",
    "print(f\"\\nWorst performing sequences (lowest accuracy):\")\n",
    "for i, (seq_idx, acc) in enumerate(sequence_accuracies[:5]):\n",
    "    true_tags = [id_to_tag[tag_id] for tag_id in y_test[seq_idx] if tag_id != 0]\n",
    "    pred_tags = [id_to_tag[tag_id] for tag_id in y_pred[seq_idx] if tag_id != 0]\n",
    "    \n",
    "    print(f\"\\nSequence {seq_idx} (Accuracy: {acc:.2f}):\")\n",
    "    print(f\"True:  {' '.join(true_tags[:20])}{'...' if len(true_tags) > 20 else ''}\")\n",
    "    print(f\"Pred:  {' '.join(pred_tags[:20])}{'...' if len(pred_tags) > 20 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze common prediction errors\n",
    "error_counts = {}\n",
    "total_errors = 0\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    true_seq = y_test[i]\n",
    "    pred_seq = y_pred[i]\n",
    "    \n",
    "    # Find positions where predictions differ (ignoring padding)\n",
    "    non_pad_mask = true_seq != 0\n",
    "    error_mask = (true_seq != pred_seq) & non_pad_mask\n",
    "    \n",
    "    for j in range(len(true_seq)):\n",
    "        if error_mask[j]:\n",
    "            true_tag = id_to_tag[true_seq[j]]\n",
    "            pred_tag = id_to_tag[pred_seq[j]]\n",
    "            error_key = f\"{true_tag} -> {pred_tag}\"\n",
    "            error_counts[error_key] = error_counts.get(error_key, 0) + 1\n",
    "            total_errors += 1\n",
    "\n",
    "# Display most common errors\n",
    "print(f\"\\nMost Common Prediction Errors (Total: {total_errors:,}):\")\n",
    "sorted_errors = sorted(error_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "for i, (error, count) in enumerate(sorted_errors[:15]):\n",
    "    percentage = (count / total_errors) * 100\n",
    "    print(f\"{i+1:2d}. {error}: {count:,} ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile comprehensive results\n",
    "advanced_results = {\n",
    "    'model_info': {\n",
    "        'model_type': 'advanced_bilstm',\n",
    "        'architecture': 'bidirectional_lstm_with_attention',\n",
    "        'parameters': model_params,\n",
    "        'training_params': training_params,\n",
    "        'training_time_seconds': training_time,\n",
    "        'total_parameters': advanced_model.model.count_params() if advanced_model.model else 0\n",
    "    },\n",
    "    'training_results': training_results,\n",
    "    'test_metrics': test_metrics,\n",
    "    'evaluation_results': {\n",
    "        'token_level': evaluation_results['token_level'],\n",
    "        'sequence_level': evaluation_results['sequence_level'],\n",
    "        'entity_level': evaluation_results['entity_level']\n",
    "    },\n",
    "    'per_tag_metrics': evaluation_results['per_tag'],\n",
    "    'error_analysis': {\n",
    "        'total_errors': total_errors,\n",
    "        'most_common_errors': dict(sorted_errors[:10])\n",
    "    },\n",
    "    'metadata': {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'data_splits': {\n",
    "            'train_size': len(X_train),\n",
    "            'val_size': len(X_val),\n",
    "            'test_size': len(X_test)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add comparison with baseline if available\n",
    "if 'baseline_results' in locals():\n",
    "    advanced_results['comparison_with_baseline'] = {\n",
    "        'improvements': improvements,\n",
    "        'baseline_metrics': baseline_results['evaluation_results']\n",
    "    }\n",
    "\n",
    "# Save results\n",
    "results_path = '../results/advanced_results.json'\n",
    "save_results(advanced_results, results_path)\n",
    "print(f\"Advanced model results saved to: {results_path}\")\n",
    "\n",
    "# Save predictions for later analysis\n",
    "predictions_path = '../results/advanced_predictions.npz'\n",
    "np.savez_compressed(\n",
    "    predictions_path,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred,\n",
    "    X_test=X_test\n",
    ")\n",
    "print(f\"Advanced predictions saved to: {predictions_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Advanced Model Summary:\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"🏗️  Architecture: Bidirectional LSTM with Dense Layers\")\n",
    "print(f\"📊 Parameters: {advanced_model.model.count_params():,}\")\n",
    "print(f\"⏱️  Training Time: {training_time/60:.1f} minutes\")\n",
    "print(f\"📈 Epochs: {training_results['epochs_trained']}\")\n",
    "\n",
    "print(f\"\\n🎯 Performance Metrics:\")\n",
    "print(f\"   • Token Accuracy: {token_metrics['accuracy']:.3f}\")\n",
    "print(f\"   • Token F1-Score: {token_metrics['f1_score']:.3f}\")\n",
    "print(f\"   • Sequence Accuracy: {seq_metrics['sequence_accuracy']:.3f}\")\n",
    "print(f\"   • Entity F1-Score: {entity_metrics['f1_score']:.3f}\")\n",
    "\n",
    "if 'improvements' in locals():\n",
    "    print(f\"\\n📈 Improvements over Baseline:\")\n",
    "    for metric, improvement in improvements.items():\n",
    "        arrow = \"📈\" if improvement > 0 else \"📉\" if improvement < 0 else \"➡️\"\n",
    "        print(f\"   {arrow} {metric}: {improvement:+.2f} percentage points\")\n",
    "\n",
    "print(f\"\\n🔍 Key Insights:\")\n",
    "best_tag = max(per_tag_metrics.keys(), key=lambda x: per_tag_metrics[x]['f1_score'])\n",
    "worst_tag = min(per_tag_metrics.keys(), key=lambda x: per_tag_metrics[x]['f1_score'])\n",
    "print(f\"   • Best performing tag: {best_tag} (F1: {per_tag_metrics[best_tag]['f1_score']:.3f})\")\n",
    "print(f\"   • Worst performing tag: {worst_tag} (F1: {per_tag_metrics[worst_tag]['f1_score']:.3f})\")\n",
    "print(f\"   • Most common error: {sorted_errors[0][0]} ({sorted_errors[0][1]:,} occurrences)\")\n",
    "\n",
    "print(f\"\\n✅ Model and results saved successfully!\")\n",
    "print(f\"📁 Files saved:\")\n",
    "print(f\"   • Model: ../models/advanced_model.h5\")\n",
    "print(f\"   • Results: ../results/advanced_results.json\")\n",
    "print(f\"   • Predictions: ../results/advanced_predictions.npz\")\n",
    "print(f\"   • Training plot: ../results/visualizations/advanced_training_history.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The advanced NER model has been successfully trained and evaluated:\n",
    "\n",
    "**Model Characteristics:**\n",
    "- Bidirectional LSTM architecture for context awareness\n",
    "- Better understanding of sequence dependencies\n",
    "- Improved handling of entity boundaries\n",
    "\n",
    "**Key Advantages over Baseline:**\n",
    "- Context-aware word representations\n",
    "- Better sequence modeling\n",
    "- Improved entity recognition capabilities\n",
    "\n",
    "**Potential Further Improvements:**\n",
    "- Add attention mechanism\n",
    "- Use pre-trained word embeddings (Word2Vec, GloVe)\n",
    "- Implement CRF layer for better sequence labeling\n",
    "- Use transformer-based models (BERT, RoBERTa)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}