{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline NER Model Training\n",
    "\n",
    "This notebook implements and trains a baseline NER model using a feedforward neural network with word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "from baseline_model import BaselineNERModel, create_baseline_model, train_baseline_model\n",
    "from data_preprocessing import NERDataProcessor\n",
    "from evaluation import NERModelEvaluator, evaluate_predictions\n",
    "from utils import load_results, save_results\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Configure TensorFlow\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "print(\"Loading preprocessed data...\")\n",
    "\n",
    "# Load the numpy arrays\n",
    "data = np.load('../results/processed_data.npz')\n",
    "X_train = data['X_train']\n",
    "X_val = data['X_val']\n",
    "X_test = data['X_test']\n",
    "y_train = data['y_train']\n",
    "y_val = data['y_val']\n",
    "y_test = data['y_test']\n",
    "\n",
    "# Load metadata\n",
    "metadata = load_results('../results/preprocessing_metadata.json')\n",
    "\n",
    "print(f\"Data loaded successfully!\")\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Vocabulary size: {metadata['vocab_size']:,}\")\n",
    "print(f\"Number of tags: {metadata['num_tags']}\")\n",
    "print(f\"Max sequence length: {metadata['max_sequence_length']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "model_params = {\n",
    "    'embedding_dim': 100,\n",
    "    'hidden_dim': 128,\n",
    "    'dropout_rate': 0.3\n",
    "}\n",
    "\n",
    "print(f\"Creating baseline model with parameters:\")\n",
    "for param, value in model_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# Create the baseline model\n",
    "baseline_model = create_baseline_model(\n",
    "    vocab_size=metadata['vocab_size'],\n",
    "    num_tags=metadata['num_tags'],\n",
    "    max_sequence_length=metadata['max_sequence_length'],\n",
    "    **model_params\n",
    ")\n",
    "\n",
    "print(\"\\nBaseline model created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and display model architecture\n",
    "baseline_model.build_model()\n",
    "print(\"Model Architecture:\")\n",
    "print(baseline_model.get_model_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "training_params = {\n",
    "    'epochs': 30,\n",
    "    'batch_size': 32,\n",
    "    'patience': 10\n",
    "}\n",
    "\n",
    "print(f\"Training parameters:\")\n",
    "for param, value in training_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Train the model\n",
    "training_results = baseline_model.train(\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    model_save_path='../models/baseline_model.h5',\n",
    "    **training_params\n",
    ")\n",
    "\n",
    "end_time = datetime.now()\n",
    "training_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "print(f\"\\nTraining completed in {training_time:.1f} seconds ({training_time/60:.1f} minutes)\")\n",
    "print(f\"Epochs trained: {training_results['epochs_trained']}\")\n",
    "print(f\"Best validation accuracy: {training_results['best_val_accuracy']:.4f}\")\n",
    "print(f\"Best validation loss: {training_results['best_val_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "baseline_model.plot_training_history(save_path='../results/visualizations/baseline_training_history.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate the Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating baseline model on test set...\")\n",
    "\n",
    "# Basic evaluation metrics\n",
    "test_metrics = baseline_model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_metrics['test_loss']:.4f}\")\n",
    "print(f\"Test Accuracy: {test_metrics['test_accuracy']:.4f}\")\n",
    "\n",
    "# Get predictions\n",
    "print(\"\\nGenerating predictions...\")\n",
    "y_pred = baseline_model.predict(X_test)\n",
    "print(f\"Predictions shape: {y_pred.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive evaluation using our custom evaluator\n",
    "print(\"Performing comprehensive evaluation...\")\n",
    "\n",
    "# Create evaluator\n",
    "id_to_tag = {int(k): v for k, v in metadata['id_to_tag'].items()}\n",
    "evaluator = NERModelEvaluator(id_to_tag)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation_results = evaluator.evaluate_model(y_test, y_pred, X_test)\n",
    "\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Token-level metrics\n",
    "token_metrics = evaluation_results['token_level']\n",
    "print(f\"Token-level Accuracy: {token_metrics['accuracy']:.4f}\")\n",
    "print(f\"Token-level Precision: {token_metrics['precision']:.4f}\")\n",
    "print(f\"Token-level Recall: {token_metrics['recall']:.4f}\")\n",
    "print(f\"Token-level F1-Score: {token_metrics['f1_score']:.4f}\")\n",
    "\n",
    "# Sequence-level metrics\n",
    "seq_metrics = evaluation_results['sequence_level']\n",
    "print(f\"\\nSequence-level Accuracy: {seq_metrics['sequence_accuracy']:.4f}\")\n",
    "print(f\"Exact Matches: {seq_metrics['exact_matches']}/{seq_metrics['total_sequences']}\")\n",
    "\n",
    "# Entity-level metrics\n",
    "entity_metrics = evaluation_results['entity_level']\n",
    "print(f\"\\nEntity-level Precision: {entity_metrics['precision']:.4f}\")\n",
    "print(f\"Entity-level Recall: {entity_metrics['recall']:.4f}\")\n",
    "print(f\"Entity-level F1-Score: {entity_metrics['f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-tag performance analysis\n",
    "per_tag_metrics = evaluation_results['per_tag']\n",
    "print(\"\\nPer-Tag Performance:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Convert to DataFrame for better display\n",
    "per_tag_df = pd.DataFrame(per_tag_metrics).T\n",
    "per_tag_df = per_tag_df.sort_values('f1_score', ascending=False)\n",
    "\n",
    "print(per_tag_df.round(4))\n",
    "\n",
    "# Plot per-tag F1 scores\n",
    "evaluator.plot_per_tag_metrics(per_tag_metrics, metric='f1_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "confusion_matrix = evaluation_results['confusion_matrix']\n",
    "evaluator.plot_confusion_matrix(confusion_matrix, title=\"Baseline Model - Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification report\n",
    "y_true_flat = evaluator._flatten_sequences(y_test)\n",
    "y_pred_flat = evaluator._flatten_sequences(y_pred)\n",
    "\n",
    "classification_report = evaluator.generate_classification_report(y_true_flat, y_pred_flat)\n",
    "print(\"Detailed Classification Report:\")\n",
    "print(\"=\" * 40)\n",
    "print(classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prediction errors\n",
    "print(\"Error Analysis:\")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "# Find sequences with lowest prediction accuracy\n",
    "sequence_accuracies = []\n",
    "for i in range(len(y_test)):\n",
    "    true_seq = y_test[i]\n",
    "    pred_seq = y_pred[i]\n",
    "    \n",
    "    # Calculate accuracy for this sequence (ignoring padding)\n",
    "    non_pad_mask = true_seq != 0  # Assuming 0 is padding\n",
    "    if non_pad_mask.sum() > 0:\n",
    "        seq_acc = (true_seq[non_pad_mask] == pred_seq[non_pad_mask]).mean()\n",
    "        sequence_accuracies.append((i, seq_acc))\n",
    "\n",
    "# Sort by accuracy\n",
    "sequence_accuracies.sort(key=lambda x: x[1])\n",
    "\n",
    "print(f\"\\nWorst performing sequences (lowest accuracy):\")\n",
    "for i, (seq_idx, acc) in enumerate(sequence_accuracies[:5]):\n",
    "    true_tags = [id_to_tag[tag_id] for tag_id in y_test[seq_idx] if tag_id != 0]\n",
    "    pred_tags = [id_to_tag[tag_id] for tag_id in y_pred[seq_idx] if tag_id != 0]\n",
    "    \n",
    "    print(f\"\\nSequence {seq_idx} (Accuracy: {acc:.2f}):\")\n",
    "    print(f\"True:  {' '.join(true_tags[:20])}{'...' if len(true_tags) > 20 else ''}\")\n",
    "    print(f\"Pred:  {' '.join(pred_tags[:20])}{'...' if len(pred_tags) > 20 else ''}\")\n",
    "\n",
    "print(f\"\\nBest performing sequences (highest accuracy):\")\n",
    "for i, (seq_idx, acc) in enumerate(sequence_accuracies[-5:]):\n",
    "    if acc < 1.0:  # Skip perfect predictions\n",
    "        true_tags = [id_to_tag[tag_id] for tag_id in y_test[seq_idx] if tag_id != 0]\n",
    "        pred_tags = [id_to_tag[tag_id] for tag_id in y_pred[seq_idx] if tag_id != 0]\n",
    "        \n",
    "        print(f\"\\nSequence {seq_idx} (Accuracy: {acc:.2f}):\")\n",
    "        print(f\"True:  {' '.join(true_tags[:20])}{'...' if len(true_tags) > 20 else ''}\")\n",
    "        print(f\"Pred:  {' '.join(pred_tags[:20])}{'...' if len(pred_tags) > 20 else ''}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze common prediction errors\n",
    "error_counts = {}\n",
    "total_errors = 0\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    true_seq = y_test[i]\n",
    "    pred_seq = y_pred[i]\n",
    "    \n",
    "    # Find positions where predictions differ (ignoring padding)\n",
    "    non_pad_mask = true_seq != 0\n",
    "    error_mask = (true_seq != pred_seq) & non_pad_mask\n",
    "    \n",
    "    for j in range(len(true_seq)):\n",
    "        if error_mask[j]:\n",
    "            true_tag = id_to_tag[true_seq[j]]\n",
    "            pred_tag = id_to_tag[pred_seq[j]]\n",
    "            error_key = f\"{true_tag} -> {pred_tag}\"\n",
    "            error_counts[error_key] = error_counts.get(error_key, 0) + 1\n",
    "            total_errors += 1\n",
    "\n",
    "# Display most common errors\n",
    "print(f\"\\nMost Common Prediction Errors (Total: {total_errors:,}):\")\n",
    "sorted_errors = sorted(error_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "for i, (error, count) in enumerate(sorted_errors[:15]):\n",
    "    percentage = (count / total_errors) * 100\n",
    "    print(f\"{i+1:2d}. {error}: {count:,} ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile comprehensive results\n",
    "baseline_results = {\n",
    "    'model_info': {\n",
    "        'model_type': 'baseline_feedforward',\n",
    "        'architecture': 'feedforward_with_embeddings',\n",
    "        'parameters': model_params,\n",
    "        'training_params': training_params,\n",
    "        'training_time_seconds': training_time,\n",
    "        'total_parameters': baseline_model.model.count_params() if baseline_model.model else 0\n",
    "    },\n",
    "    'training_results': training_results,\n",
    "    'test_metrics': test_metrics,\n",
    "    'evaluation_results': {\n",
    "        'token_level': evaluation_results['token_level'],\n",
    "        'sequence_level': evaluation_results['sequence_level'],\n",
    "        'entity_level': evaluation_results['entity_level']\n",
    "    },\n",
    "    'per_tag_metrics': evaluation_results['per_tag'],\n",
    "    'error_analysis': {\n",
    "        'total_errors': total_errors,\n",
    "        'most_common_errors': dict(sorted_errors[:10])\n",
    "    },\n",
    "    'metadata': {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'data_splits': {\n",
    "            'train_size': len(X_train),\n",
    "            'val_size': len(X_val),\n",
    "            'test_size': len(X_test)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save results\n",
    "results_path = '../results/baseline_results.json'\n",
    "save_results(baseline_results, results_path)\n",
    "print(f\"Baseline model results saved to: {results_path}\")\n",
    "\n",
    "# Save predictions for later analysis\n",
    "predictions_path = '../results/baseline_predictions.npz'\n",
    "np.savez_compressed(\n",
    "    predictions_path,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred,\n",
    "    X_test=X_test\n",
    ")\n",
    "print(f\"Baseline predictions saved to: {predictions_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline Model Summary:\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"🏗️  Architecture: Feedforward Neural Network with Embeddings\")\n",
    "print(f\"📊 Parameters: {baseline_model.model.count_params():,}\")\n",
    "print(f\"⏱️  Training Time: {training_time/60:.1f} minutes\")\n",
    "print(f\"📈 Epochs: {training_results['epochs_trained']}\")\n",
    "\n",
    "print(f\"\\n🎯 Performance Metrics:\")\n",
    "print(f\"   • Token Accuracy: {token_metrics['accuracy']:.3f}\")\n",
    "print(f\"   • Token F1-Score: {token_metrics['f1_score']:.3f}\")\n",
    "print(f\"   • Sequence Accuracy: {seq_metrics['sequence_accuracy']:.3f}\")\n",
    "print(f\"   • Entity F1-Score: {entity_metrics['f1_score']:.3f}\")\n",
    "\n",
    "print(f\"\\n🔍 Key Insights:\")\n",
    "best_tag = max(per_tag_metrics.keys(), key=lambda x: per_tag_metrics[x]['f1_score'])\n",
    "worst_tag = min(per_tag_metrics.keys(), key=lambda x: per_tag_metrics[x]['f1_score'])\n",
    "print(f\"   • Best performing tag: {best_tag} (F1: {per_tag_metrics[best_tag]['f1_score']:.3f})\")\n",
    "print(f\"   • Worst performing tag: {worst_tag} (F1: {per_tag_metrics[worst_tag]['f1_score']:.3f})\")\n",
    "print(f\"   • Most common error: {sorted_errors[0][0]} ({sorted_errors[0][1]:,} occurrences)\")\n",
    "\n",
    "print(f\"\\n✅ Model and results saved successfully!\")\n",
    "print(f\"📁 Files saved:\")\n",
    "print(f\"   • Model: ../models/baseline_model.h5\")\n",
    "print(f\"   • Results: ../results/baseline_results.json\")\n",
    "print(f\"   • Predictions: ../results/baseline_predictions.npz\")\n",
    "print(f\"   • Training plot: ../results/visualizations/baseline_training_history.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The baseline NER model has been successfully trained and evaluated:\n",
    "\n",
    "**Model Characteristics:**\n",
    "- Simple feedforward architecture with word embeddings\n",
    "- No context awareness between words\n",
    "- Limited understanding of sequence dependencies\n",
    "\n",
    "**Expected Limitations:**\n",
    "- Poor handling of unseen words\n",
    "- Difficulty with entity boundaries\n",
    "- No consideration of word order or context\n",
    "\n",
    "**Next Steps:**\n",
    "- Compare with advanced BiLSTM model\n",
    "- Analyze areas for improvement\n",
    "- Use insights to guide advanced model development"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}