{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing for NER Dataset\n",
    "\n",
    "This notebook handles data preprocessing including:\n",
    "- Sentence reconstruction\n",
    "- Vocabulary building\n",
    "- Sequence encoding and padding\n",
    "- Train/validation/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "from data_preprocessing import NERDataProcessor, analyze_dataset\n",
    "from utils import (\n",
    "    load_dataset, get_unique_tags, save_results,\n",
    "    print_dataset_info, plot_tag_distribution\n",
    ")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_path = '../data/ner_dataset.csv'\n",
    "df = load_dataset(data_path)\n",
    "\n",
    "print(f\"Dataset loaded: {df.shape[0]:,} tokens across {df['Sentence #'].nunique():,} sentences\")\n",
    "print(f\"Unique words: {df['Word'].nunique():,}\")\n",
    "print(f\"Unique tags: {df['Tag'].nunique()}\")\n",
    "\n",
    "# Show sample\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Data Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the data processor\n",
    "# Set max_sequence_length based on sentence length analysis\n",
    "sentence_lengths = df.groupby('Sentence #').size()\n",
    "max_length = int(sentence_lengths.quantile(0.95))  # Use 95th percentile\n",
    "\n",
    "print(f\"Sentence length statistics:\")\n",
    "print(f\"Mean: {sentence_lengths.mean():.1f}\")\n",
    "print(f\"Median: {sentence_lengths.median():.1f}\")\n",
    "print(f\"95th percentile: {max_length}\")\n",
    "print(f\"Max: {sentence_lengths.max()}\")\n",
    "\n",
    "# Initialize processor with the calculated max length\n",
    "processor = NERDataProcessor(max_sequence_length=max_length)\n",
    "print(f\"\\nInitialized NERDataProcessor with max_sequence_length={max_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the complete dataset\n",
    "print(\"Processing dataset...\")\n",
    "processed_data = processor.process_data(data_path)\n",
    "\n",
    "print(\"\\nProcessing completed!\")\n",
    "print(f\"Vocabulary size: {processed_data['metadata']['vocab_size']:,}\")\n",
    "print(f\"Number of tags: {processed_data['metadata']['num_tags']}\")\n",
    "print(f\"Max sequence length: {processed_data['metadata']['max_sequence_length']}\")\n",
    "\n",
    "# Extract data splits\n",
    "X_train = processed_data['X_train']\n",
    "X_val = processed_data['X_val']\n",
    "X_test = processed_data['X_test']\n",
    "y_train = processed_data['y_train']\n",
    "y_val = processed_data['y_val']\n",
    "y_test = processed_data['y_test']\n",
    "metadata = processed_data['metadata']\n",
    "\n",
    "print(f\"\\nData splits:\")\n",
    "print(f\"Training: {X_train.shape[0]:,} sequences\")\n",
    "print(f\"Validation: {X_val.shape[0]:,} sequences\")\n",
    "print(f\"Test: {X_test.shape[0]:,} sequences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Examine Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the shape and properties of processed data\n",
    "print(\"Training data shapes:\")\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "\n",
    "print(f\"\\nSample input sequence (first 10 tokens):\")\n",
    "print(X_train[0][:10])\n",
    "\n",
    "print(f\"\\nSample output sequence (first 10 tags):\")\n",
    "print(y_train[0][:10])\n",
    "\n",
    "# Convert back to readable format for inspection\n",
    "id_to_tag = metadata['id_to_tag']\n",
    "sample_tags = [id_to_tag[str(tag_id)] for tag_id in y_train[0][:20]]\n",
    "print(f\"\\nSample tags (readable): {sample_tags}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine tag distribution in processed data\n",
    "tag_to_id = metadata['tag_to_id']\n",
    "print(\"Tag mappings:\")\n",
    "for tag, tag_id in sorted(tag_to_id.items(), key=lambda x: x[1]):\n",
    "    print(f\"{tag}: {tag_id}\")\n",
    "\n",
    "# Calculate tag distribution in training data\n",
    "train_tag_counts = Counter(y_train.flatten())\n",
    "print(\"\\nTag distribution in training data:\")\n",
    "for tag_id, count in sorted(train_tag_counts.items()):\n",
    "    tag_name = id_to_tag[str(tag_id)]\n",
    "    percentage = (count / len(y_train.flatten())) * 100\n",
    "    print(f\"{tag_name} (ID {tag_id}): {count:,} ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Sequence Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze padding in sequences\n",
    "pad_token_id = 0  # Assuming PAD token has ID 0\n",
    "\n",
    "def count_non_pad_tokens(sequences, pad_id=0):\n",
    "    \"\"\"Count non-padding tokens in each sequence.\"\"\"\n",
    "    return [(seq != pad_id).sum() for seq in sequences]\n",
    "\n",
    "train_lengths = count_non_pad_tokens(X_train)\n",
    "val_lengths = count_non_pad_tokens(X_val)\n",
    "test_lengths = count_non_pad_tokens(X_test)\n",
    "\n",
    "print(\"Actual sequence lengths (excluding padding):\")\n",
    "print(f\"Training - Mean: {np.mean(train_lengths):.1f}, Max: {np.max(train_lengths)}, Min: {np.min(train_lengths)}\")\n",
    "print(f\"Validation - Mean: {np.mean(val_lengths):.1f}, Max: {np.max(val_lengths)}, Min: {np.min(val_lengths)}\")\n",
    "print(f\"Test - Mean: {np.mean(test_lengths):.1f}, Max: {np.max(test_lengths)}, Min: {np.min(test_lengths)}\")\n",
    "\n",
    "# Plot sequence length distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(train_lengths, bins=50, alpha=0.7, label='Training', density=True)\n",
    "plt.hist(val_lengths, bins=50, alpha=0.7, label='Validation', density=True)\n",
    "plt.hist(test_lengths, bins=50, alpha=0.7, label='Test', density=True)\n",
    "plt.xlabel('Actual Sequence Length')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Distribution of Actual Sequence Lengths')\n",
    "plt.legend()\n",
    "plt.axvline(max_length, color='red', linestyle='--', label=f'Max Length ({max_length})')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how much data is truncated\n",
    "truncated_count = sum(1 for length in sentence_lengths if length > max_length)\n",
    "truncation_percentage = (truncated_count / len(sentence_lengths)) * 100\n",
    "\n",
    "print(f\"Sequences longer than max_length ({max_length}): {truncated_count:,} ({truncation_percentage:.2f}%)\")\n",
    "print(f\"Sequences kept fully: {len(sentence_lengths) - truncated_count:,} ({100 - truncation_percentage:.2f}%)\")\n",
    "\n",
    "if truncation_percentage > 5:\n",
    "    print(\"⚠️  Warning: More than 5% of sequences are truncated. Consider increasing max_sequence_length.\")\n",
    "else:\n",
    "    print(\"✅ Good: Less than 5% of sequences are truncated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for saving\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "\n",
    "# Save the data processor\n",
    "processor_path = '../models/data_processor.pkl'\n",
    "processor.save_processor(processor_path)\n",
    "print(f\"Data processor saved to: {processor_path}\")\n",
    "\n",
    "# Save processed data\n",
    "data_save_path = '../results/processed_data.npz'\n",
    "np.savez_compressed(\n",
    "    data_save_path,\n",
    "    X_train=X_train,\n",
    "    X_val=X_val,\n",
    "    X_test=X_test,\n",
    "    y_train=y_train,\n",
    "    y_val=y_val,\n",
    "    y_test=y_test\n",
    ")\n",
    "print(f\"Processed data saved to: {data_save_path}\")\n",
    "\n",
    "# Save metadata\n",
    "metadata_path = '../results/preprocessing_metadata.json'\n",
    "save_results(metadata, metadata_path)\n",
    "print(f\"Metadata saved to: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Validation and Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate data integrity\n",
    "print(\"Data Validation:\")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "# Check shapes\n",
    "assert X_train.shape[0] == y_train.shape[0], \"Training X and y have different number of samples\"\n",
    "assert X_val.shape[0] == y_val.shape[0], \"Validation X and y have different number of samples\"\n",
    "assert X_test.shape[0] == y_test.shape[0], \"Test X and y have different number of samples\"\n",
    "print(\"✅ Shape consistency check passed\")\n",
    "\n",
    "# Check sequence lengths\n",
    "assert X_train.shape[1] == max_length, \"Training sequences have wrong length\"\n",
    "assert y_train.shape[1] == max_length, \"Training labels have wrong length\"\n",
    "print(\"✅ Sequence length check passed\")\n",
    "\n",
    "# Check value ranges\n",
    "assert X_train.min() >= 0, \"Negative values in X_train\"\n",
    "assert X_train.max() < metadata['vocab_size'], \"X_train values exceed vocab size\"\n",
    "assert y_train.min() >= 0, \"Negative values in y_train\"\n",
    "assert y_train.max() < metadata['num_tags'], \"y_train values exceed num_tags\"\n",
    "print(\"✅ Value range check passed\")\n",
    "\n",
    "# Check data splits don't overlap\n",
    "total_samples = X_train.shape[0] + X_val.shape[0] + X_test.shape[0]\n",
    "expected_samples = metadata['total_sentences']\n",
    "assert total_samples == expected_samples, f\"Data split error: {total_samples} != {expected_samples}\"\n",
    "print(\"✅ Data split check passed\")\n",
    "\n",
    "print(\"\\n🎉 All validation checks passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the processor with a sample text\n",
    "sample_text = \"Barack Obama visited New York yesterday.\"\n",
    "print(f\"Testing processor with sample text: '{sample_text}'\")\n",
    "\n",
    "# Preprocess the text\n",
    "processed_sample = processor.preprocess_text(sample_text)\n",
    "print(f\"Processed shape: {processed_sample.shape}\")\n",
    "print(f\"Processed sequence (first 10 tokens): {processed_sample[0][:10]}\")\n",
    "\n",
    "# Show the word mappings for this text\n",
    "words = sample_text.split()\n",
    "word_ids = [processor.word_to_id.get(word, processor.word_to_id['<UNK>']) for word in words]\n",
    "print(f\"\\nWord to ID mapping:\")\n",
    "for word, word_id in zip(words, word_ids):\n",
    "    status = \"(UNK)\" if word_id == processor.word_to_id['<UNK>'] else \"\"\n",
    "    print(f\"  {word}: {word_id} {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary statistics\n",
    "summary_stats = {\n",
    "    'dataset_info': {\n",
    "        'total_tokens': int(df.shape[0]),\n",
    "        'total_sentences': int(metadata['total_sentences']),\n",
    "        'vocabulary_size': int(metadata['vocab_size']),\n",
    "        'num_tags': int(metadata['num_tags']),\n",
    "        'max_sequence_length': int(metadata['max_sequence_length'])\n",
    "    },\n",
    "    'data_splits': {\n",
    "        'train_samples': int(X_train.shape[0]),\n",
    "        'val_samples': int(X_val.shape[0]),\n",
    "        'test_samples': int(X_test.shape[0]),\n",
    "        'train_percentage': round((X_train.shape[0] / total_samples) * 100, 1),\n",
    "        'val_percentage': round((X_val.shape[0] / total_samples) * 100, 1),\n",
    "        'test_percentage': round((X_test.shape[0] / total_samples) * 100, 1)\n",
    "    },\n",
    "    'sequence_stats': {\n",
    "        'mean_train_length': round(np.mean(train_lengths), 1),\n",
    "        'mean_val_length': round(np.mean(val_lengths), 1),\n",
    "        'mean_test_length': round(np.mean(test_lengths), 1),\n",
    "        'truncation_percentage': round(truncation_percentage, 2)\n",
    "    },\n",
    "    'tag_distribution': {id_to_tag[str(tag_id)]: int(count) for tag_id, count in train_tag_counts.items()}\n",
    "}\n",
    "\n",
    "# Save summary statistics\n",
    "summary_path = '../results/preprocessing_summary.json'\n",
    "save_results(summary_stats, summary_path)\n",
    "print(f\"Summary statistics saved to: {summary_path}\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\nPreprocessing Summary:\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"📊 Dataset: {summary_stats['dataset_info']['total_tokens']:,} tokens, {summary_stats['dataset_info']['total_sentences']:,} sentences\")\n",
    "print(f\"📚 Vocabulary: {summary_stats['dataset_info']['vocabulary_size']:,} unique words\")\n",
    "print(f\"🏷️  Tags: {summary_stats['dataset_info']['num_tags']} unique NER tags\")\n",
    "print(f\"📏 Max Length: {summary_stats['dataset_info']['max_sequence_length']} tokens\")\n",
    "print(f\"\\n🔄 Data Splits:\")\n",
    "print(f\"   • Training: {summary_stats['data_splits']['train_samples']:,} ({summary_stats['data_splits']['train_percentage']}%)\")\n",
    "print(f\"   • Validation: {summary_stats['data_splits']['val_samples']:,} ({summary_stats['data_splits']['val_percentage']}%)\")\n",
    "print(f\"   • Test: {summary_stats['data_splits']['test_samples']:,} ({summary_stats['data_splits']['test_percentage']}%)\")\n",
    "print(f\"\\n✂️ Truncation: {summary_stats['sequence_stats']['truncation_percentage']}% of sequences truncated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Data preprocessing completed successfully!\n",
    "\n",
    "**What we accomplished:**\n",
    "1. ✅ Loaded and analyzed the raw NER dataset\n",
    "2. ✅ Built vocabulary mappings for words and tags\n",
    "3. ✅ Converted text sequences to numerical representations\n",
    "4. ✅ Applied appropriate padding/truncation\n",
    "5. ✅ Split data into train/validation/test sets (60%/20%/20%)\n",
    "6. ✅ Validated data integrity and quality\n",
    "7. ✅ Saved processed data and metadata for model training\n",
    "\n",
    "**Next steps:**\n",
    "- The processed data is ready for model training\n",
    "- Both baseline and advanced models can use this preprocessed data\n",
    "- All mappings and metadata are preserved for consistent evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}