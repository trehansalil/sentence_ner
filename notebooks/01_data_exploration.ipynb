{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration for NER Dataset\n",
    "\n",
    "This notebook explores the Named Entity Recognition (NER) dataset to understand its structure, distribution, and characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "from utils import (\n",
    "    load_dataset, get_unique_tags, get_tag_distribution,\n",
    "    plot_tag_distribution, print_dataset_info,\n",
    "    get_sentence_length_stats, plot_sentence_length_distribution\n",
    ")\n",
    "from data_preprocessing import analyze_dataset\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Basic Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_path = '../data/ner_dataset.csv'\n",
    "df = load_dataset(data_path)\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset information\n",
    "print_dataset_info(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. NER Tags Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique tags and their distribution\n",
    "unique_tags = get_unique_tags(df)\n",
    "tag_distribution = get_tag_distribution(df)\n",
    "\n",
    "print(f\"Number of unique NER tags: {len(unique_tags)}\")\n",
    "print(f\"Unique tags: {unique_tags}\")\n",
    "\n",
    "print(\"\\nTag distribution:\")\n",
    "for tag, count in sorted(tag_distribution.items(), key=lambda x: x[1], reverse=True):\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"{tag}: {count:,} ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tag distribution\n",
    "plot_tag_distribution(tag_distribution, \"Distribution of NER Tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze entity types (B- and I- tags)\n",
    "entity_tags = [tag for tag in unique_tags if tag.startswith(('B-', 'I-'))]\n",
    "entity_types = set(tag[2:] for tag in entity_tags if len(tag) > 2)\n",
    "\n",
    "print(f\"Number of entity types: {len(entity_types)}\")\n",
    "print(f\"Entity types: {sorted(entity_types)}\")\n",
    "\n",
    "# Count entities by type\n",
    "entity_type_counts = {}\n",
    "for entity_type in entity_types:\n",
    "    b_count = df[df['Tag'] == f'B-{entity_type}'].shape[0]\n",
    "    i_count = df[df['Tag'] == f'I-{entity_type}'].shape[0]\n",
    "    entity_type_counts[entity_type] = {'B': b_count, 'I': i_count, 'Total': b_count + i_count}\n",
    "\n",
    "entity_df = pd.DataFrame(entity_type_counts).T\n",
    "entity_df = entity_df.sort_values('Total', ascending=False)\n",
    "print(\"\\nEntity type distribution:\")\n",
    "print(entity_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot entity types distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "x = range(len(entity_df))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar([i - width/2 for i in x], entity_df['B'], width, label='B- (Beginning)', alpha=0.8)\n",
    "plt.bar([i + width/2 for i in x], entity_df['I'], width, label='I- (Inside)', alpha=0.8)\n",
    "\n",
    "plt.xlabel('Entity Types')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of B- and I- Tags by Entity Type')\n",
    "plt.xticks(x, entity_df.index, rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sentence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence length statistics\n",
    "sentence_stats = get_sentence_length_stats(df)\n",
    "\n",
    "print(\"Sentence length statistics:\")\n",
    "for stat, value in sentence_stats.items():\n",
    "    print(f\"{stat.replace('_', ' ').title()}: {value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sentence length distribution\n",
    "plot_sentence_length_distribution(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze some sample sentences\n",
    "print(\"Sample sentences with their tags:\")\n",
    "for sent_id in df['Sentence #'].unique()[:5]:\n",
    "    sent_data = df[df['Sentence #'] == sent_id]\n",
    "    words = sent_data['Word'].tolist()\n",
    "    tags = sent_data['Tag'].tolist()\n",
    "    \n",
    "    print(f\"\\n{sent_id}:\")\n",
    "    print(f\"Words: {' '.join(words)}\")\n",
    "    print(f\"Tags:  {' '.join(tags)}\")\n",
    "    \n",
    "    # Extract entities from this sentence\n",
    "    from utils import extract_entities\n",
    "    entities = extract_entities(words, tags)\n",
    "    if entities:\n",
    "        print(f\"Entities: {entities}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vocabulary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word frequency analysis\n",
    "word_counts = df['Word'].value_counts()\n",
    "\n",
    "print(f\"Total vocabulary size: {len(word_counts)}\")\n",
    "print(f\"Most frequent words:\")\n",
    "print(word_counts.head(20))\n",
    "\n",
    "print(f\"\\nLeast frequent words (sample):\")\n",
    "print(word_counts.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot word frequency distribution (top 30)\n",
    "plt.figure(figsize=(15, 6))\n",
    "top_words = word_counts.head(30)\n",
    "plt.bar(range(len(top_words)), top_words.values)\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Top 30 Most Frequent Words')\n",
    "plt.xticks(range(len(top_words)), top_words.index, rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze word frequency distribution\n",
    "freq_counts = word_counts.value_counts().sort_index()\n",
    "\n",
    "print(\"Word frequency distribution:\")\n",
    "print(f\"Words appearing once: {freq_counts[1] if 1 in freq_counts else 0}\")\n",
    "print(f\"Words appearing 2-5 times: {freq_counts[2:6].sum() if len(freq_counts) > 2 else 0}\")\n",
    "print(f\"Words appearing 6-10 times: {freq_counts[6:11].sum() if len(freq_counts) > 6 else 0}\")\n",
    "print(f\"Words appearing >10 times: {freq_counts[11:].sum() if len(freq_counts) > 11 else 0}\")\n",
    "\n",
    "# Plot frequency of frequencies\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(word_counts.values, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Word Frequency')\n",
    "plt.ylabel('Number of Words')\n",
    "plt.title('Distribution of Word Frequencies')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comprehensive Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the comprehensive analysis function\n",
    "analysis_results = analyze_dataset(df)\n",
    "\n",
    "print(\"Comprehensive Dataset Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for key, value in analysis_results.items():\n",
    "    print(f\"\\n{key.replace('_', ' ').title()}:\")\n",
    "    if isinstance(value, dict):\n",
    "        for sub_key, sub_value in value.items():\n",
    "            print(f\"  {sub_key}: {sub_value}\")\n",
    "    else:\n",
    "        print(f\"  {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for potential data quality issues\n",
    "print(\"Data Quality Assessment:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Check for consecutive B- tags of the same type (potential issue)\n",
    "consecutive_b_issues = 0\n",
    "for sent_id in df['Sentence #'].unique()[:1000]:  # Check first 1000 sentences\n",
    "    sent_data = df[df['Sentence #'] == sent_id]\n",
    "    tags = sent_data['Tag'].tolist()\n",
    "    \n",
    "    for i in range(len(tags) - 1):\n",
    "        if tags[i].startswith('B-') and tags[i+1].startswith('B-') and tags[i] == tags[i+1]:\n",
    "            consecutive_b_issues += 1\n",
    "            if consecutive_b_issues <= 5:  # Show first 5 examples\n",
    "                print(f\"Consecutive B- tags in {sent_id}: {tags[i]} -> {tags[i+1]}\")\n",
    "\n",
    "print(f\"\\nFound {consecutive_b_issues} potential consecutive B- tag issues in first 1000 sentences\")\n",
    "\n",
    "# Check for I- tags without preceding B- tags\n",
    "orphan_i_issues = 0\n",
    "for sent_id in df['Sentence #'].unique()[:1000]:  # Check first 1000 sentences\n",
    "    sent_data = df[df['Sentence #'] == sent_id]\n",
    "    tags = sent_data['Tag'].tolist()\n",
    "    \n",
    "    for i, tag in enumerate(tags):\n",
    "        if tag.startswith('I-'):\n",
    "            entity_type = tag[2:]\n",
    "            # Check if there's a B- tag before this I- tag\n",
    "            has_b_tag = False\n",
    "            for j in range(i-1, -1, -1):\n",
    "                if tags[j] == f'B-{entity_type}':\n",
    "                    has_b_tag = True\n",
    "                    break\n",
    "                elif tags[j] != f'I-{entity_type}':\n",
    "                    break\n",
    "            \n",
    "            if not has_b_tag:\n",
    "                orphan_i_issues += 1\n",
    "                if orphan_i_issues <= 5:  # Show first 5 examples\n",
    "                    print(f\"Orphan I- tag in {sent_id}: {tag} at position {i}\")\n",
    "\n",
    "print(f\"Found {orphan_i_issues} potential orphan I- tag issues in first 1000 sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset Summary and Key Insights:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(f\"ðŸ“Š Dataset Size: {df.shape[0]:,} tokens across {df['Sentence #'].nunique():,} sentences\")\n",
    "print(f\"ðŸ“ Vocabulary: {df['Word'].nunique():,} unique words\")\n",
    "print(f\"ðŸ·ï¸  NER Tags: {df['Tag'].nunique()} unique tags\")\n",
    "print(f\"ðŸŽ¯ Entity Types: {len(entity_types)} types ({', '.join(sorted(entity_types))})\")\n",
    "\n",
    "print(f\"\\nðŸ“ Sentence Lengths:\")\n",
    "print(f\"   â€¢ Average: {sentence_stats['mean_length']:.1f} words\")\n",
    "print(f\"   â€¢ Range: {sentence_stats['min_length']} - {sentence_stats['max_length']} words\")\n",
    "print(f\"   â€¢ Median: {sentence_stats['median_length']:.1f} words\")\n",
    "\n",
    "o_percentage = (tag_distribution['O'] / len(df)) * 100\n",
    "entity_percentage = 100 - o_percentage\n",
    "print(f\"\\nðŸ” Tag Distribution:\")\n",
    "print(f\"   â€¢ Non-entity tokens (O): {o_percentage:.1f}%\")\n",
    "print(f\"   â€¢ Entity tokens: {entity_percentage:.1f}%\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ Key Insights:\")\n",
    "print(f\"   â€¢ This is a {['small', 'medium', 'large'][2 if df.shape[0] > 100000 else 1 if df.shape[0] > 10000 else 0]} dataset\")\n",
    "print(f\"   â€¢ Entity density: {'High' if entity_percentage > 30 else 'Medium' if entity_percentage > 15 else 'Low'}\")\n",
    "print(f\"   â€¢ Vocabulary richness: {'High' if df['Word'].nunique() > 20000 else 'Medium' if df['Word'].nunique() > 5000 else 'Low'}\")\n",
    "print(f\"   â€¢ Sentence complexity: {'High' if sentence_stats['mean_length'] > 25 else 'Medium' if sentence_stats['mean_length'] > 15 else 'Low'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This exploration provides a comprehensive understanding of the NER dataset:\n",
    "\n",
    "1. **Dataset Scale**: Large-scale dataset suitable for training robust NER models\n",
    "2. **Entity Diversity**: Multiple entity types with varying frequencies\n",
    "3. **Text Complexity**: Diverse sentence lengths and vocabulary\n",
    "4. **Data Quality**: Generally well-formatted IOB2 tagging with minimal issues\n",
    "\n",
    "The insights from this exploration will guide the preprocessing steps and model architecture decisions in subsequent notebooks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}